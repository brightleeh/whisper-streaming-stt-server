model:
  name: "small" # Whisper model size: tiny | base | small | medium | large | large-v1 | large-v2 | large-v3
  backend: "faster_whisper" # faster_whisper | torch_whisper
  device: "cpu" # cpu / cuda / mps (torch_whisper)
  compute_type: "int8" # backend compute type: int8 | int8_float16 | float16 | float32 (recommend: int8)
  pool_size: 1 # Number of preloaded model instances

  # Language configuration
  language_fix: false
  language: "" # empty = auto; set when language_fix is true (e.g., de/en/ja/ko/zh)
  task: "transcribe" # "transcribe" | "translate" (keep "transcribe" to avoid translation)

  # Default profile when clients do not request one
  default_decode_profile: "realtime"
  default_model_load_profile: "default" # Default profile for /admin/load_model

# Optional admin load profiles for /admin/load_model (profile_id)
# If omitted, a default profile is derived from the "model" section above.
model_load_profiles:
  default: &profile_default
    model_size: "small"
    backend: "faster_whisper"
    device: "cpu"
    compute_type: "int8"
    pool_size: 1
  nvidia_cuda:
    <<: *profile_default
    device: "cuda"
    compute_type: "float16"
  apple_silicon_mps:
    <<: *profile_default
    backend: "torch_whisper"
    device: "mps"
    compute_type: "float16"

# Decode profiles picked per client request
decode_profiles:
  # Real-time profile (matches the current low-latency setup)
  realtime:
    beam_size: 1
    best_of: 1
    patience: 1.0
    temperature: 0.0
    length_penalty: 1.0
    without_timestamps: true # Keep true if the server does not emit timestamps
    compression_ratio_threshold: 2.4
    no_speech_threshold: 0.6
    log_prob_threshold: -1.0 # Low-probability rejection threshold (-1.0 disables)

  # Accuracy-first profile (batch/file STT or post-processing)
  accurate:
    beam_size: 5
    best_of: 5
    patience: 1.0
    temperature: 0.0
    length_penalty: 1.0
    without_timestamps: true
    compression_ratio_threshold: 2.4
    no_speech_threshold: 0.6
    log_prob_threshold: -1.0
